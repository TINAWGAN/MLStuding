## Introduction
# In this data analysis report, I usually focus on feature visualization and selection 
# as a different from other kernels. Feature selection with correlation, univariate feature selection, 
# recursive feature elimination, recursive feature elimination with cross validation and 
# tree based feature selection methods are used with random forest classification. 
# Apart from these, principle component analysis are used to observe number of components.


## Data Analysis Content
#. [Python Libraries](#1)
#. [Data Content](#2)
#. [Read and Analyse Data](#3)
#. [Visualization](#4)
#. [Feature Selection and Random Forest Classification](#5)
    #. [Feature selection with correlation and random forest classification](#6)
    #. [Univariate feature selection and random forest classification](#7)
    #. [Recursive feature elimination (RFE) with random forest](#8)
    #. [Recursive feature elimination with cross validation and random forest classification](#9)
    #. [Tree based feature selection and random forest classification](#10)
#. [Feature Extraction with PCA](#11)
#. [Conclusion](#12)



import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # data visualization library  
import matplotlib.pyplot as plt
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
import time
from subprocess import check_output
print(check_output(["ls", "../input"]).decode("utf8"))
#import warnings library
import warnings
# ignore all warnings
warnings.filterwarnings('ignore')


## Data Content
# 1. **ID number**
# 1. **Diagnosis (M = malignant, B = benign)**
# 1. **radius (mean of distances from center to points on the perimeter)**
# 1. **texture (standard deviation of gray-scale values)**
# 1. **perimeter**
# 1. **area**
# 1. **smoothness (local variation in radius lengths)**
# 1. **compactness (perimeter^2 / area - 1.0)**
# 1. **concavity (severity of concave portions of the contour)**
# 1. **concave points (number of concave portions of the contour)**
# 1. **symmetry**
# 1. **fractal dimension ("coastline approximation" - 1)**
* The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.
* All feature values are recoded with four significant digits.
* Missing attribute values: none
* Class distribution: 357 benign, 212 malignant
# Read n analysis data
data = pd.read_csv('../input/data.csv')
# Before making anything like feature selection,feature extraction and classification, firstly we start with basic data analysis. 
Lets look at features of data.

data.head()  # head method show only first 5 rows
# **There are 4 things that take my attention**
# 1) There is an **id** that cannot be used for classificaiton 
# 2) **Diagnosis** is our class label
# 3) **Unnamed: 32** feature includes NaN so we do not need it.
# 4) I do not have any idea about other feature names actually I do not need because machine learning is awesome **:)**

# Therefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!
col = data.columns       # .columns gives columns names in data 
print(col)
# y includes our labels and x includes our features
y = data.diagnosis                          # M or B 
list = ['Unnamed: 32','id','diagnosis']
x = data.drop(list,axis = 1 )
x.head()
ax = sns.countplot(y,label="Count")       # M = 212, B = 357
B, M = y.value_counts()
print('Number of Benign: ',B)
print('Number of Malignant : ',M)

# Okey, now we have features but **what does they mean** or actually **how much do we need to know about these features**
# The answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values.
# These type of information helps to understand about what is going on data.
# For example , the question is appeared in my mind the **area_mean** feature's max value is 2500 and smoothness_mean 
# features' max 0.16340. Therefore **do we need standardization or normalization before visualization, 
# feature selection, feature extraction or classificaiton?** The answer is yes and no not surprising ha :) 
# Anyway lets go step by step and start with visualization.  


x.describe()

# vvisualization Before violin and swarm plot we need to normalization or standirdization. 
# Because differences between values of features are very high to observe on plot. 
# I plot features in 3 group and each group includes 10 features to observe better.
# first ten features
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
# Second ten features
data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)

# Second ten features
data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="diagnosis", data=data,split=True, inner="quart")
plt.xticks(rotation=90)


# As an alternative of violin plot, box plot can be used
# box plots are also useful in terms of seeing outliers
# I do not visualize all features with box plot
# In order to show you lets have an example of box plot
# If you want, you can visualize other features as well.
plt.figure(figsize=(10,10))
sns.boxplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)

sns.jointplot(x.loc[:,'concavity_worst'], x.loc[:,'concave points_worst'], kind="regg", color="#ce1414")

# What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)
# And we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection.


sns.set(style="white")
df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]
g = sns.PairGrid(df, diag_sharey=False)
g.map_lower(sns.kdeplot, cmap="Blues_d")
g.map_upper(plt.scatter)
g.map_diag(sns.kdeplot, lw=3)

sns.set(style="whitegrid", palette="muted")
data_dia = y
data = x
data_n_2 = (data - data.mean()) / (data.std())              # standardization
data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
tic = time.time()
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)

plt.xticks(rotation=90)


data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)

data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)
data = pd.melt(data,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)
toc = time.time()
plt.xticks(rotation=90)
print("swarm plot time: ", toc-tic ," s")


# They looks cool right. And you can see variance more clear. 
# Let me ask you a question, **in these three plots which feature looks like more clear in terms of classification.
# In my opinion **area_worst** in last swarm plot looks like malignant and benign are seprated not totaly but mostly. 
# Hovewer, **smoothness_se** in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while 
# using this feature.

#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)

## Feature Selection and Random Forest Classification
# Today our purpuse is to try new cocktails. For example, we are finaly in the pub and we want to drink different tastes. 
# Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to 
# eliminate other drinks which includes lemon so as to experience very different tastes.
# In this part we will select feature with different methods that are feature selection with correlation, 
# univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation 
# (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and 
# predict. 

### 1) Feature selection with correlation and random forest classification
# As it can be seen in map heat figure radius_mean, perimeter_mean and area_mean are correlated with each other 
# so we will use only **area_mean**. If you ask how i choose **area_mean** as a feature to use, well actually there 
# is no correct answer, I just look at swarm plots and **area_mean** looks like clear for me but we cannot make exact 
# separation among other correlated features without trying. So lets find other correlated features and look accuracy 
# with random forest classifier. 
# Compactness_mean, concavity_mean and concave points_mean** are correlated with each other.Therefore I only choose 
# concavity_mean. Apart from these, **radius_se, perimeter_se and area_se** are correlated and I only use area_se 
# radius_worst, perimeter_worst and area_worst** are correlated so I use **area_worst**.  **Compactness_worst, 
# concavity_worst and concave points_worst** so I use **concavity_worst**.  **Compactness_se, concavity_se and 
# concave points_se** so I use **concavity_se**. **texture_mean and texture_worst are correlated** and I use
# texture_mean**. **area_worst and area_mean** are correlated, I use **area_mean**.

drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']
x_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later 
x_1.head()
#correlation map
f,ax = plt.subplots(figsize=(14, 14))
sns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)


from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score,confusion_matrix
from sklearn.metrics import accuracy_score

# split data train 70 % and test 30 %
x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)

#random forest classifier with n_estimators=10 (default)
clf_rf = RandomForestClassifier(random_state=43)      
clr_rf = clf_rf.fit(x_train,y_train)

ac = accuracy_score(y_test,clf_rf.predict(x_test))
print('Accuracy is: ',ac)
cm = confusion_matrix(y_test,clf_rf.predict(x_test))
sns.heatmap(cm,annot=True,fmt="d")

# Accuracy is:  0.9532163742690059
# Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. 

### 2) Univariate feature selection and random forest classification
# In univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.
# In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 
# 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and 
# find best 5 features.

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
# find best scored 5 features
select_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)

print('Score list:', select_feature.scores_)
print('Feature list:', x_train.columns)

# Best 5 feature to classify is that area_mean, area_se, texture_mean, concavity_worst and concavity_mean
# So lets se what happens if we use only these best scored 5 feature.

x_train_2 = select_feature.transform(x_train)
x_test_2 = select_feature.transform(x_test)
#random forest classifier with n_estimators=10 (default)
clf_rf_2 = RandomForestClassifier()      
clr_rf_2 = clf_rf_2.fit(x_train_2,y_train)
ac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))
print('Accuracy is: ',ac_2)
cm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))
sns.heatmap(cm_2,annot=True,fmt="d")
# Accuracy is:  0.9298245614035088

### 3) Recursive feature elimination (RFE) with random forest
# Basically, it uses one of the classification methods (random forest in our example), assign weights to each of features.
# Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively 
# repeated on the pruned set until the desired number of features

# Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.
from sklearn.feature_selection import RFE
# Create the RFE object and rank each pixel
clf_rf_3 = RandomForestClassifier()      
rfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)
rfe = rfe.fit(x_train, y_train)
print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])

#Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**.
# They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. 
# Shortly, we can say that we make good feature selection with rfe and selectkBest methods. 
# However as you can see there is a problem, okey I except we find best 5 feature with two different method and these 
# features are same but why it is **5**. Maybe if we use best 2 or best 15 feature we will have better accuracy. 
# Therefore lets see how many feature we need to use with rfecv method.

### 4) Recursive feature elimination with cross validation and random forest classification
from sklearn.feature_selection import RFECV

# The "accuracy" scoring is proportional to the number of correct classifications
clf_rf_4 = RandomForestClassifier() 
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)

print('Optimal number of features :', rfecv.n_features_)
print('Best features :', x_train.columns[rfecv.support_])
# Optimal number of features : 8
# Best features : Index(['texture_mean', 'area_mean', 'concavity_mean', 'fractal_dimension_mean',
       'area_se', 'symmetry_se', 'concavity_worst', 'symmetry_worst'],
      dtype='object')
      
      
# Plot number of features VS. cross-validation scores
import matplotlib.pyplot as plt
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score of number of selected features")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

#  Our purpose is learning how to make **feature selection and understanding data. Then last make our last feature selection method.
### 5) Tree based feature selection and random forest classification
# In random forest classification method there is a **feature_importances_** attributes that is the feature importances 
# (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should 
# not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance 
# list can change.

clf_rf_5 = RandomForestClassifier()      
clr_rf_5 = clf_rf_5.fit(x_train,y_train)
importances = clr_rf_5.feature_importances_
std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(x_train.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest

plt.figure(1, figsize=(14, 13))
plt.title("Feature importances")
plt.bar(range(x_train.shape[1]), importances[indices],
       color="g", yerr=std[indices], align="center")
plt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)
plt.xlim([-1, x_train.shape[1]])
plt.show()

## Feature Extraction with PCA
#  Before PCA, we need to normalize data for better performance of PCA.
# split data train 70 % and test 30 %
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#normalization
x_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())
x_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(x_train_N)

plt.figure(1, figsize=(14, 13))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_ratio_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_ratio_')

# According to variance ration, 3 component can be chosen.














